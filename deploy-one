#!/bin/sh

# FIXME
version=1.0.0

if [ $UID -ne 0 ]; then
	echo "pls run as root!"
	exit 1
fi

cd `dirname $0`
top=$PWD

opcode=deploy

while [ $# -gt 0 ]
do
	case $1 in
	-d|--destroy)
		opcode=destroy
		;;
	-i|--id)
		this=$2
		shift
		;;
	-k|--key)
		pk=$2
		shift
	*)
		echo "option '$1' not supported!"
		exit 1
	esac
	shift
done

#hadoop_repo="repo.maxwit.com:/mnt/witpub/cloud/apache"
hadoop_repo="/mnt/witpub/cloud/apache"

hadoop_user=hadoop
hadoop_user_home="/var/lib/$hadoop_user"
data_root="$hadoop_user_home/data"

util_root="/opt/util"
mdh_root="/opt/mdh"

echo "###########################################"
echo "                 MDH v$version"
echo "    MaxWit Distribution of Apache Hadoop"
echo "###########################################"
echo

. ./.config

hive_url=$hadoop_repo/apache-hive-${hive_version}-bin.tar.gz
hadoop_url=$hadoop_repo/hadoop-${hadoop_version}tar.gz
hbase_url=$hadoop_repo/hbase-${hbase_version}-bin.tar.gz
pig_url=$hadoop_repo/pig-${pig_version}tar.gz
spark_url=$hadoop_repo/spark-${spark_version}-bin-hadoop2.6.tgz
zookeeper_url=$hadoop_repo/zookeeper-${zookeeper_version}tar.gz

# FIX the url
java_url=/mnt/witpub/devel/java/jdk/jdk-${java_version}-linux-x64.tar.gz
scala_url=/mnt/witpub/devel/scala/scala-${scala_version}.tgz

slaves=(${config_slaves//,/ })

if [ "${#slaves[@]}" -gt 0 ]; then
	mode="cluster"
	master=$config_master
else
	mode="pseudo"
	master="localhost"
fi

hosts=($master $config_slaves)

if [ -e /etc/redhat-release ]; then
	profile="/etc/bash_profile.d"
else
	profile="/etc/profile.d"
fi
profile="$profile/mdh.sh"

function add_env
{
	local key=$1
	local val=$2

	echo "export $key=$val" >> $profile
	#grep -w $key $profile > /dev/null
	#if [ $? -eq 0 ]; then
	#	sed -i "s:$key=.*:$key=$val:" $profile
	#else
	#	echo "export $key=$val" >> $profile
	#fi

	#if [ $? -ne 0 ]; then
	#	echo "fail to export $key!"
	#	exit 1
	#fi
}

function del_env
{
	local key=$1

	sed -i "/export $key/d" $profile
}

function check_profile_path
{
	local path=$1

	grep "PATH=$path:" $profile > /dev/null || \
		grep "PATH=.*:$path$" $profile > /dev/null || \
			grep "PATH=.*:$path:" $profile > /dev/null

	return $?
}

function add_path
{
	local path=$1

	echo "export PATH=$path:\$PATH" >> $profile
	#check_profile_path $path || echo "export PATH=$path:\$PATH" >> $profile
	#check_profile_path $path || exit 1
}

function del_path
{
	local path=$1
	check_profile_path $path && sed -i "#PATH=.*$path#d" $profile

	local NEW_PATH=`echo $PATH | sed -e 's#$path:##' | sed -e 's#:$path##'`
}

function download_and_extract
{
	local app=$1
	local url=$"$app"_url
	local dest=$"$app"_home
	local downloads=`mktemp -d`
	#local tarball=`basename $url`

	#scp $url $downloads || return 1
	echo -n "downloading and extracting $tarball ."
	cd $downloads
	tar xf $url || return 1
	echo -n "."
 	mv * $dest || return 1
	echo -n "."
	rm -rf $downloads
	echo " done"
}

function execute
{
	func=$1

	echo "*****************************************"
	echo "  executing $func() ..."
	echo "*****************************************"

	$func
	if [ $? -ne 0 ]; then
		echo "fail to run $func!"
		exit 1
	fi

	echo
}

log="/tmp/mdh-`date +%H%M%S`.log"
echo "# MDH Log (`date`)" > $log
echo >> $log

app_seq=""
app_rev=""

for app in hadoop zookeeper hbase spark
do
	ver=$"$app"_version
	if [ -n "$ver" ]; then
		app_seq="$app_seq $app"
		app_rev="$app $app_rev"

		"$app"_home=$mdh_root/$app

		module="module/$app.sh"
		if [ ! -e $module ]; then
			echo "$module does not exists!"
			exit 1
		fi
		. $module
	fi
done

util_list=""
for util in java scala pig
do
	ver=$"$util"_version
	if [ -n "$ver" ]; then
		"$util"_home=$util_root/$util
		util_list="$util_list $util"
	fi
done

if [ $opcode = deploy]; then
	echo "MDH will be installed to '$mdh_root'"

	grep "^$hadoop_user:" /etc/passwd && \
	{
		echo "user '$hadoop_user' already exists!"
		exit 1
	}
	useradd -m -d $hadoop_user_home -s /bin/bash -c "Hadoop" $hadoop_user
	
	sudo -i -u $hadoop_user mkdir .ssh
	if [ ${hosts[$this]} == $master ]; then
		sudo -i -u $hadoop_user cp $pk .ssh && \
		sudo -i -u $hadoop_user cp ${pk%.pub} .ssh || exit 1
	fi
	sudo -i -u $hadoop_user cp $pk .ssh/authorized_keys
	sudo -i -u $hadoop_user chmod 700 -R .ssh
	
	sudo -u $hadoop_user mkdir -p $data_root || exit 1

	mkdir -p $util_root || exit 1

	for util in $util_list
	do
		download_and_extract $util
		home=`echo $util | tr a-z A-Z`_HOME
		add_env $home $"$util"_home
		add_path "\$$home/bin"
	done

	mkdir -p $mdh_root || exit 1
	
	for app in $app_seq
	do
		download_and_extract $app || exit 1

		cd $"$app"_home || exit 1
		execute ${app}_deploy || exit 1

		service=/etc/init.d/$app
		home=`echo $app | tr a-z A-Z`_HOME
		cat > $service << EOF
#!/bin/sh

$home=$"$app"_home

EOF
		cat $top/service/$app > $service
		chmod +x $service

		echo
	done
else
	echo "MDH destroying ..."

	grep "^$hadoop_user:" /etc/passwd || \
	{
		echo "hadoop not installed!"
		exit 0
	}

	for app in $app_rev
	do
		rm -f /etc/init.d/$app
		execute ${app}_destroy || exit 1
		rm -rf $"$app"_home
		rm -f /etc/init.d/$app
		echo
	done

	rm -rf $mdh_root || exit 1
	sudo -u $hadoop_user rm -rf $data_root

	userdel -r $hadoop_user
fi

echo "MDH ${opcode}ed successfully!"
echo
