#!/bin/sh

if [ $UID -ne 0 ]; then
	echo "pls run as root!"
	exit 1
fi

# FIXME
version=1.0.0

#hadoop_repo="repo.maxwit.com:/mnt/witpub/cloud/apache"
hadoop_repo="/mnt/witpub/cloud/apache"

cd `dirname $0`
top=$PWD

tmp_dir=`mktemp -d`

hadoop_user=hadoop
hadoop_user_home="/var/lib/$hadoop_user"
data_root="$hadoop_user_home/data"

mdh_root="/opt/mdh"

ssh_setup()
{
	kt="rsa"
	kf="id_$kt"

	if [ ! -f ~/.ssh/${kf} ]; then
		ssh-keygen -P '' -f ~/.ssh/${kf}
	fi

	#key=`cat ~/.ssh/${kf}.pub`
	#grep "$key" ~/.ssh/authorized_keys > /dev/null 2>&1|| echo "$key" >> ~/.ssh/authorized_keys

	total=${#hosts[@]}
	count=1

	for host in ${hosts[@]}
	do
		echo "Copying $kf [$count/$total]: $host ..."
		# TODO: no-interactive support
		ssh-copy-id $host

		((count++))
		echo
	done

	for host in ${hosts[@]}
	do
		ssh $host echo "login $host successfully!"
	done
	echo
}

if [ $# -ne 1 ]; then
	echo "usage: `basename $0` <init|deploy|destroy|test|start|stop>"
	exit 1
fi

echo "###########################################"
echo "                 MDH v$version"
echo "    MaxWit Distribution of Apache Hadoop"
echo "###########################################"
echo

if [ -e ./.config ]; then
	. ./.config
fi

hive_url=$hadoop_repo/apache-hive-${hive_version}-bin.tar.gz
hadoop_url=$hadoop_repo/hadoop-${hadoop_version}tar.gz
hbase_url=$hadoop_repo/hbase-${hbase_version}-bin.tar.gz
pig_url=$hadoop_repo/pig-${pig_version}tar.gz
spark_url=$hadoop_repo/spark-${spark_version}-bin-hadoop2.6.tgz
zookeeper_url=$hadoop_repo/zookeeper-${zookeeper_version}tar.gz

slaves=($config_slaves)

if [ "${#slaves[@]}" -gt 0 ]; then
	mode="cluster"
	master=$config_master
	my_name=$1
else
	mode="pseudo"
	master="localhost"
	my_name=`hostname`
fi

hosts=($master $config_slaves)

echo "Deploying MDH ($mode mode)"
if [ $mode = 'cluster' ]; then
	echo "Cluster nodes:"
	i=0
	for host in ${hosts[@]}
	do
		((i++))
		echo "[$i] $host"
	done
fi
echo

if [ $cmd = 'init' ]; then
	ssh_setup
	exit $?
fi

if [ -z "$JAVA_HOME" ]; then
	echo -e "JAVA_HOME not set!\n"
	exit 1
fi

if [ -e /etc/redhat-release ]; then
	profile="/etc/bash_profile.d"
	installer=yum
else
	profile="/etc/profile.d"
	installer=apt-get
fi
profile="$profile/hadoop.sh"

$installer install -y rsync openssh-server || exit 1

add_env()
{
	local key=$1
	local val=$2

	echo "export $key=$val" >> $profile
	#grep -w $key $profile > /dev/null
	#if [ $? -eq 0 ]; then
	#	sed -i "s:$key=.*:$key=$val:" $profile
	#else
	#	echo "export $key=$val" >> $profile
	#fi

	#if [ $? -ne 0 ]; then
	#	echo "fail to export $key!"
	#	exit 1
	#fi

	#eval export $key=$val
}

del_env()
{
	local key=$1

	sed -i "/export $key/d" $profile
	unset $key
}

check_profile_path()
{
	local path=$1

	grep "PATH=$path:" $profile > /dev/null || \
		grep "PATH=.*:$path$" $profile > /dev/null || \
			grep "PATH=.*:$path:" $profile > /dev/null

	return $?
}

add_path()
{
	local path=$1

	echo "export PATH=$path:\$PATH" >> $profile
	#check_profile_path $path || echo "export PATH=$path:\$PATH" >> $profile
	#check_profile_path $path || exit 1

	## FIXME
	#echo $PATH | grep -w $path || \
	#	eval export PATH="$path:\$PATH"
}

del_path()
{
	local path=$1
	check_profile_path $path && sed -i "#PATH=.*$path#d" $profile

	local NEW_PATH=`echo $PATH | sed -e 's#$path:##' | sed -e 's#:$path##'`
	eval export PATH=$NEW_PATH
}

extract()
{
	local app=$1
	local url=$"$app"_url
	#local tarball=`basename $url`
	local dest=$mdh_root/$app

	#scp $url $tmp_dir || return 1
	echo -n "extracting $tarball -> $dest . "
	cd $tmp_dir
	tar xf $url || return 1
	echo -n "."
 	mv *$app* $dest || return 1
	echo -n "."
	cd $dest
	echo " done"
}

execute()
{
	func=$1

	echo "*****************************************"
	echo "  executing $func() ..."
	echo "*****************************************"

	$func
	if [ $? -ne 0 ]; then
		echo "fail to run $func!"
		exit 1
	fi

	echo
}

log="/tmp/mdh-`date +%H%M%S`.log"
echo "# MDH Log (`date`)" > $log
echo >> $log

apps=""
rev_apps=""

for app in hadoop hive pig zookeeper hbase spark
do
	app_ver=$"$app"_version
	if [ -n "$app_ver" ]; then
		apps="$apps $app"
		rev_apps="$app $rev_apps"
	fi
done

for app in $apps
do
	module="module/$app.sh"
	if [ ! -e $module ]; then
		echo "$module does not exists!"
		exit 1
	fi

	. $module
done

echo "MDH will be installed to '$mdh_root'"

grep "^$hadoop_user:" /etc/passwd && \
{
	echo "user '$hadoop_user' already exists!"
	exit 1
}
useradd -m -d $hadoop_user_home -s /bin/bash -c "Hadoop" $hadoop_user

sudo -i -u $hadoop_user mkdir .ssh
sudo -i -u $hadoop_user chmod 700 .ssh
sudo -i -u $hadoop_user ssh-keygen -P '' -f .ssh/id_rsa -C $my_name

sudo -u $hadoop_user mkdir -p $data_root || exit 1
mkdir -p $mdh_root || exit 1
# chown $hadoop_user.$hadoop_user $mdh_root || exit 1

cmd=`basename $0`
cmd=${cmd%-one}

if [ $cmd = deploy]; then
	for app in $apps
	do
		extract $app
		execute ${app}_deploy || exit 1

		APP="echo $app | tr a-z A-Z"
		sed "s:${APP}_HOME=.*:${APP}_HOME=$mdh_root/$app:" \
			$top/service/$app > /etc/init.d/$app
		chmod a+x /etc/init.d/$app

		/etc/init.d/$app start

		if [ $app = hadoop ]; then
			hadoop_init
		fi

		echo
	done
else
	grep "^$hadoop_user:" /etc/passwd || \
	{
		echo "hadoop not installed!"
		exit 0
	}

	for app in $rev_apps
	do
		/etc/init.d/$app stop
		rm -f /etc/init.d/$app

		cd $top
		execute ${app}_destroy || exit 1

		rm -rf $mdh_root/$app
		echo
	done

	rm -rf $mdh_root || exit 1
	sudo -u $hadoop_user rm -rf $data_root
	userdel -r $hadoop_user
fi

echo "MDH ${cmd}ed successfully!"
echo
